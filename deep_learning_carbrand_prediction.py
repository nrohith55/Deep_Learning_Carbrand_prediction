# -*- coding: utf-8 -*-
"""Deep_Learning_Carbrand_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P6rwYbHRCG-pe2s9JDz_LaFjpJ3f4LOS
"""

import numpy as np
import matplotlib.pyplot as plt
from glob import glob

from tensorflow.keras.models import Sequential,Model,load_model

from tensorflow.keras.layers import Input,Lambda,Dense,Flatten

from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img

from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input

from tensorflow.keras.preprocessing import image

#Resizeing all the images
IMAGE_SIZE=[224,224]
train_path=('/content/drive/My Drive/Deep learning/Datasets/Train')
valid_path=('/content/drive/My Drive/Deep learning/Datasets/Test')

resnet=ResNet50(input_shape=IMAGE_SIZE+[3],weights='imagenet',include_top=False)
# Here we import the Resnet50 library as shown below and add preprocessing layer to the front of VGG
# Here we will be using imagenet weights

#donot train the existing weights
for layer in resnet.layers:
  layer.trainable=False

folders=glob('/content/drive/My Drive/Deep learning/Datasets/Train/*')

folders

resnet.summary()

# our layers - you can add more if you want
x=Flatten()(resnet.output)

len(folders)

prediction=Dense(len(folders),activation='softmax')(x)

#Create a model
model=Model(inputs=resnet.input,outputs=prediction)

model.summary()

# tell the model what cost and optimization method to use
model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

# Use the Image Data Generator to import the images from the dataset
training_datagen=ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)

test_datagen=ImageDataGenerator(rescale=1./255)

## Make sure you provide the same target size as initialied for the image size
training_set=training_datagen.flow_from_directory('/content/drive/My Drive/Deep learning/Datasets/Train',
                                                  target_size=(224,224),
                                                  class_mode='categorical',
                                                  batch_size=32)

test_set=test_datagen.flow_from_directory('/content/drive/My Drive/Deep learning/Datasets/Test',
                                          target_size=(224,224),batch_size=32,
                                          class_mode='categorical')

# fit the model
# Run the cell. It will take some time to execute
r=model.fit_generator (training_set,validation_data=test_set,
                      epochs=50,
                      steps_per_epoch=len(training_set),
                      validation_steps=len(test_set))

# plot the loss
plt.plot(r.history['loss'], label='train loss')
plt.plot(r.history['val_loss'], label='val loss')
plt.legend()
plt.show()
plt.savefig('LossVal_loss')

# plot the accuracy
plt.plot(r.history['accuracy'], label='train acc')
plt.plot(r.history['val_accuracy'], label='val acc')
plt.legend()
plt.show()
plt.savefig('AccVal_acc')

model.save('model_resnet50.h5')

y_pred=model.predict(test_set)

y_pred=np.argmax(y_pred,axis=1)

model=load_model('model_resnet50.h5')

img=image.load_img('/content/drive/My Drive/Deep learning/Datasets/Test/lamborghini/10.jpg',target_size=(224,224))

x=image.img_to_array(img)
x

x.shape

x=x/255

x=np.expand_dims(x,axis=0)
img_data=preprocess_input(x)
img_data.shape

model.predict(img_data)

a=np.argmax(model.predict(img_data), axis=1)

a==1

